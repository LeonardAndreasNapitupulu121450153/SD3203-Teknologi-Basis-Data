# -*- coding: utf-8 -*-
"""121450153_RA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NCgonCeKgqszYZxUJZEnneU27jXWOuRT

## A Dataset to Play With
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pickle
from pathlib import Path

# Path to the unzipped CIFAR data
data_dir = Path("/content/drive/MyDrive/cifar-10-batches-py")

# Unpickle function provided by the CIFAR hosts
def unpickle(file):
    with open(file, "rb") as fo:
        dict = pickle.load(fo, encoding="bytes")
    return dict

images, labels = [], []
for batch in data_dir.glob("data_batch_*"):
    batch_data = unpickle(batch)
    for i, flat_im in enumerate(batch_data[b"data"]):
        im_channels = []
        # Each image is flattened, with channels in order of R, G, B
        for j in range(3):
            im_channels.append(
                flat_im[j * 1024 : (j + 1) * 1024].reshape((32, 32))
            )
        # Reconstruct the original image
        images.append(np.dstack((im_channels)))
        # Save the label
        labels.append(batch_data[b"labels"][i])

print("Loaded CIFAR-10 training set:")
print(f" - np.shape(images)     {np.shape(images)}")
print(f" - np.shape(labels)     {np.shape(labels)}")

"""Kode ini bertujuan untuk memuat dataset CIFAR-10 ke dalam list `images` dan `labels`. Dilakukan impor library yang diperlukan seperti numpy, pickle, dan Path dari pathlib. Selanjutnya, path ke direktori data CIFAR yang telah di-unzip ditentukan. Kemudian, fungsi `unpickle(file)` dibuat untuk membaca file batch CIFAR menggunakan pickle. Selanjutnya, list kosong `images` dan `labels` diinisialisasi untuk menyimpan gambar dan label dari dataset CIFAR.

Dilakukan iterasi untuk setiap file batch dalam direktori data CIFAR, dan menggunakan fungsi `unpickle()` untuk membaca data dari setiap batch. Setiap gambar dalam batch diubah kembali menjadi matriks 32x32 untuk setiap channel (R, G, B) dalam gambar yang sudah di-flatten.

Gambar yang telah direkonstruksi disimpan ke dalam list `images` dan label ke dalam list `labels`. Terakhir, informasi tentang dataset CIFAR-10 yang telah dimuat, seperti bentuk array `images` dan `labels`, ditampilkan menggunakan `np.shape()`. Dengan demikian, kode berhasil memuat dataset CIFAR-10 ke dalam list `images` dan `labels` untuk digunakan dalam pelatihan model.

## Storing a Single Image
"""

from pathlib import Path

disk_dir = Path("data/disk/")
lmdb_dir = Path("data/lmdb/")
hdf5_dir = Path("data/hdf5/")

disk_dir.mkdir(parents=True, exist_ok=True)
lmdb_dir.mkdir(parents=True, exist_ok=True)
hdf5_dir.mkdir(parents=True, exist_ok=True)

"""Kode tersebut bertujuan untuk membuat struktur direktori penyimpanan data dalam tiga format yang berbeda: disk, LMDB (Lightning Memory-Mapped Database), dan HDF5 (Hierarchical Data Format version 5). Pertama, modul `Path` dari library `pathlib` diimpor untuk memungkinkan manipulasi path file. Selanjutnya, tiga objek `disk_dir`, `lmdb_dir`, dan `hdf5_dir` dibuat menggunakan `Path` untuk mewakili direktori penyimpanan untuk masing-masing format. Kemudian, metode `mkdir(parents=True, exist_ok=True)` digunakan untuk membuat direktori-direktori tersebut. Argumen `parents=True` digunakan agar direktori induk juga dibuat jika belum ada, sedangkan `exist_ok=True` memastikan bahwa pembuatan direktori tidak akan menyebabkan exception jika direktori tersebut sudah ada. Dengan demikian, kode berhasil menciptakan struktur direktori penyimpanan yang diperlukan untuk menyimpan data dalam berbagai format sesuai dengan kebutuhan pengolahan data.

## Storing to Disk
"""

from PIL import Image
import csv

def store_single_disk(image, image_id, label):
    """ Stores a single image as a .png file on disk.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    Image.fromarray(image).save(disk_dir / f"{image_id}.png")

    with open(disk_dir / f"{image_id}.csv", "wt") as csvfile:
        writer = csv.writer(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        writer.writerow([label])

"""Fungsi `store_single_disk` dalam kode tersebut bertujuan untuk menyimpan sebuah gambar tunggal ke dalam format file .png dan .csv pada disk. Pertama, baris pertama menggunakan modul `Image` dari library PIL (Python Imaging Library) untuk mengonversi array gambar (`image`) menjadi objek gambar, yang kemudian disimpan sebagai file .png dengan nama berdasarkan `image_id` ke dalam direktori yang telah ditentukan sebelumnya yaitu `disk_dir`.

Selanjutnya, dengan menggunakan modul `csv`, fungsi membuka file .csv dengan nama berdasarkan `image_id` di dalam direktori `disk_dir` untuk ditulis. Kemudian, objek writer dibuat untuk menulis data ke file .csv dengan pengaturan spesifik seperti underscore dll. Terakhir, fungsi menulis satu baris data ke file .csv yang berisi label dari gambar yang disimpan, sesuai dengan `image_id`.

Jadi gambar dan labelnya berhasil disimpan ke dalam file .png dan .csv di dalam direktori disk yang telah disiapkan sebelumnya.

##Storing to LMDB
"""

class CIFAR_Image:
    def __init__(self, image, label):
        # Dimensions of image for reconstruction - not really necessary
        # for this dataset, but some datasets may include images of
        # varying sizes
        self.channels = image.shape[2]
        self.size = image.shape[:2]

        self.image = image.tobytes()
        self.label = label

    def get_image(self):
        """ Returns the image as a numpy array. """
        image = np.frombuffer(self.image, dtype=np.uint8)
        return image.reshape(*self.size, self.channels)

!pip install lmdb

import lmdb
import pickle

def store_single_lmdb(image, image_id, label):
    """ Stores a single image to a LMDB.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    map_size = image.nbytes * 10

    # Create a new LMDB environment
    env = lmdb.open(str(lmdb_dir / f"single_lmdb"), map_size=map_size)

    # Start a new write transaction
    with env.begin(write=True) as txn:
        # All key-value pairs need to be strings
        value = CIFAR_Image(image, label)
        key = f"{image_id:08}"
        txn.put(key.encode("ascii"), pickle.dumps(value))
    env.close()

"""Fungsi `store_single_lmdb` dalam kode tersebut bertujuan untuk menyimpan sebuah gambar tunggal ke dalam format LMDB (Lightning Memory-Mapped Database). Pertama, ukuran peta memori (`map_size`) dihitung berdasarkan ukuran byte dari array gambar (`image`) dikalikan dengan faktor 10. Selanjutnya, lingkungan LMDB baru dibuat dengan menggunakan direktori `lmdb_dir` dan nama "single_lmdb", serta parameter `map_size` untuk menentukan ukuran maksimum peta memori yang dialokasikan.

Kemudian, dengan menggunakan pernyataan `with`, fungsi memulai transaksi tulis baru di lingkungan LMDB. Objek `value` dibuat yang berisi data gambar dan label, kemudian kunci unik dalam bentuk string dengan panjang tetap 8 karakter dihasilkan berdasarkan `image_id`. Selanjutnya, pasangan kunci-nilai disimpan ke dalam transaksi LMDB setelah proses pengubahan kunci menjadi format byte ASCII dan nilai di-serialize menggunakan modul `pickle`.

Terakhir, lingkungan LMDB ditutup setelah transaksi selesai untuk memastikan data tersimpan dengan baik. Dengan demikian, fungsi `store_single_lmdb` berhasil menyimpan gambar dan labelnya ke dalam format LMDB dengan menggunakan LMDB environment, transaksi tulis, dan penyimpanan data dalam bentuk kunci-nilai.

##Storing With HDF5
"""

import h5py

def store_single_hdf5(image, image_id, label):
    """ Stores a single image to an HDF5 file.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    # Create a new HDF5 file
    file = h5py.File(hdf5_dir / f"{image_id}.h5", "w")

    # Create a dataset in the file
    dataset = file.create_dataset(
        "image", np.shape(image), h5py.h5t.STD_U8BE, data=image
    )
    meta_set = file.create_dataset(
        "meta", np.shape(label), h5py.h5t.STD_U8BE, data=label
    )
    file.close()

"""Untuk menyimpan gambar dengan labelnya ke dalam file HDF5, fungsi store_single_hdf5 menerima tiga parameter: image, yang merupakan array gambar dengan dimensi (32, 32, 3), image_id, yang merupakan ID khusus untuk gambar, dan label, yang merupakan label gambar.

Untuk memulai, fungsi menggunakan modul h5py untuk membuat dan berinteraksi dengan file HDF5. Fungsi h5py.File membuat file HDF5 baru dengan nama file yang ditentukan (hdf5_dir / f"{image_id}.h5").

Di dalam file HDF5 juga dibuat dua dataset: satu untuk data gambar ("image") dan satu lagi untuk label ("meta"). Untuk tujuan ini, metode create_dataset digunakan, yang menetapkan nama, bentuk, tipe data, dan data yang akan disimpan. Tipe data yang digunakan adalah h5py.h5t.STD_U8BE untuk data 8-bit yang tidak disignifikasi.

Terakhir, file HDF5 ditutup dengan file.close(). Dengan menggunakan fungsi store_single_hdf5, gambar beserta labelnya dapat disimpan ke dalam sebuah file HDF5

##Experiments for Storing a Single Image
"""

_store_single_funcs = dict(
    disk=store_single_disk, lmdb=store_single_lmdb, hdf5=store_single_hdf5
)

from timeit import timeit

store_single_timings = dict()

for method in ("disk", "lmdb", "hdf5"):
    t = timeit(
        "_store_single_funcs[method](image, 0, label)",
        setup="image=images[0]; label=labels[0]",
        number=1,
        globals=globals(),
    )
    store_single_timings[method] = t
    print(f"Method: {method}, Time usage: {t}")

"""Kode ini dapat digunakan untuk membandingkan waktu yang dibutuhkan untuk menyimpan gambar dan labelnya ke dalam file HDF5, LMDB, dan disk. Pertama, kamus _store_single_funcs dibuat untuk menyimpan fungsi-fungsi yang digunakan untuk menyimpan gambar dalam format apa pun.
Selanjutnya, waktu eksekusi dari setiap metode penyimpanan dihitung untuk gambar pertama dalam dataset menggunakan modul timeit. Loop for digunakan untuk melakukan pengukuran untuk setiap metode penyimpanan.

Hasil eksperimen menunjukkan bahwa penyimpanan file HDF5 (hdf5) memiliki waktu eksekusi yang paling cepat, diikuti oleh LMDB (lmdb), dan penyimpanan ke dalam disk (disk) memiliki waktu eksekusi yang paling lama. Ini menunjukkan bahwa pilihan format penyimpanan dapat mempengaruhi kinerja aplikasi, terutama dalam hal jumlah waktu yang dibutuhkan untuk proses pengambilan dan penyimpanan data. Dengan memahami fitur masing-masing format penyimpanan.

##Storing Many Images
"""

def store_many_disk(images, labels):
    """ Stores an array of images to disk
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
    """
    num_images = len(images)

    # Save all the images one by one
    for i, image in enumerate(images):
        Image.fromarray(image).save(disk_dir / f"{i}.png")

    # Save all the labels to the csv file
    with open(disk_dir / f"{num_images}.csv", "w") as csvfile:
        writer = csv.writer(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        for label in labels:
            # This typically would be more than just one value per row
            writer.writerow([label])

def store_many_lmdb(images, labels):
    """ Stores an array of images to LMDB.
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
    """
    num_images = len(images)

    map_size = num_images * images[0].nbytes * 10

    # Create a new LMDB DB for all the images
    env = lmdb.open(str(lmdb_dir / f"{num_images}_lmdb"), map_size=map_size)

    # Same as before — but let's write all the images in a single transaction
    with env.begin(write=True) as txn:
        for i in range(num_images):
            # All key-value pairs need to be Strings
            value = CIFAR_Image(images[i], labels[i])
            key = f"{i:08}"
            txn.put(key.encode("ascii"), pickle.dumps(value))
    env.close()

def store_many_hdf5(images, labels):
    """ Stores an array of images to HDF5.
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
    """
    num_images = len(images)

    # Create a new HDF5 file
    file = h5py.File(hdf5_dir / f"{num_images}_many.h5", "w")

    # Create a dataset in the file
    dataset = file.create_dataset(
        "images", np.shape(images), h5py.h5t.STD_U8BE, data=images
    )
    meta_set = file.create_dataset(
        "meta", np.shape(labels), h5py.h5t.STD_U8BE, data=labels
    )
    file.close()

"""Fungsi `store_many_disk` digunakan untuk menyimpan array gambar ke disk dalam format PNG dan menyimpan labelnya dalam file CSV. Pertama, fungsi menghitung jumlah gambar yang akan disimpan. Kemudian, setiap gambar disimpan satu per satu ke disk menggunakan loop, dan label-labelnya disimpan dalam file CSV.

Fungsi `store_many_lmdb` digunakan untuk menyimpan array gambar ke LMDB (Lightning Memory-Mapped Database). Fungsi ini juga menghitung jumlah gambar yang akan disimpan serta menghitung ukuran peta memori yang diperlukan. Selanjutnya, fungsi membuat database LMDB baru untuk semua gambar dan menulis semua gambar dalam satu transaksi.

Fungsi `store_many_hdf5` digunakan untuk menyimpan array gambar ke HDF5 (Hierarchical Data Format version 5). Fungsi ini juga menghitung jumlah gambar yang akan disimpan dan membuat file HDF5 baru.

##Preparing the Dataset
"""

cutoffs = [10, 100, 1000, 10000, 100000]

# Let's double our images so that we have 100,000
images = np.concatenate((images, images), axis=0)
labels = np.concatenate((labels, labels), axis=0)

# Make sure you actually have 100,000 images and labels
print(np.shape(images))
print(np.shape(labels))

"""Sebuah dataset yang cukup besar untuk pelatihan model dibuat dengan kode ini. Pertama, variabel cutoff harus didefinisikan, yang menentukan batas jumlah data yang diinginkan. Setelah itu, perintah diberikan untuk menggandakan jumlah gambar dalam dataset hingga 100,000. Fungsi np.concatenate digunakan untuk melakukan proses penggandaan, yang menggabungkan array gambar dan label ke dalam diri mereka sendiri. Ini dilakukan dengan menggabungkan array itu sendiri dengan sumbu 0 (axis=0), yang berarti bahwa data baru akan ditambahkan ke baris baru setelah data sebelumnya. Untuk memastikan bahwa jumlah data telah mencapai target 100,000, bentuk array gambar dan label dicetak. Hasil cetakan menunjukkan bentuk array gambar adalah (100000, 32, 32, 3), yang menunjukkan bahwa terdapat 100,000 gambar dengan dimensi 32 x 32 pixel dan tiga saluran warna (RGB), sementara bentuk array label adalah 100000, yang menunjukkan bahwa ada 100,000 label yang sesuai dengan gambar tersebut.

##Experiment for Storing Many Images
"""

_store_many_funcs = dict(
    disk=store_many_disk, lmdb=store_many_lmdb, hdf5=store_many_hdf5
)

from timeit import timeit

store_many_timings = {"disk": [], "lmdb": [], "hdf5": []}

for cutoff in cutoffs:
    for method in ("disk", "lmdb", "hdf5"):
        t = timeit(
            "_store_many_funcs[method](images_, labels_)",
            setup="images_=images[:cutoff]; labels_=labels[:cutoff]",
            number=1,
            globals=globals(),
        )
        store_many_timings[method].append(t)

        # Print out the method, cutoff, and elapsed time
        print(f"Method: {method}, Time usage: {t}")

"""Kode di atas membuat sebuah perintah yang berisi fungsi-fungsi untuk menyimpan data ke berbagai jenis penyimpanan, yaitu disk, lmdb, dan hdf5. Kemudian, dilakukan pengukuran waktu eksekusi untuk setiap metode penyimpanan dengan menggunakan jumlah data yang berbeda-beda.
Pertama, kamus _store_many_funcs dibuat untuk menyimpan fungsi-fungsi penyimpanan dengan kunci berupa jenis penyimpanan dan nilai berupa fungsi yang sesuai.
Selanjutnya, dilakukan pengukuran waktu eksekusi untuk setiap metode penyimpanan dengan menggunakan jumlah data yang berbeda-beda. Loop pertama akan melakukan iterasi terhadap nilai-nilai cutoffs, yang mungkin merupakan suatu variabel yang didefinisikan sebelumnya. Loop kedua akan melakukan iterasi terhadap metode penyimpanan, yaitu “disk”, “lmdb”, dan “hdf5”.
Setelah itu, dilakukan pengukuran waktu eksekusi untuk setiap metode penyimpanan dengan menggunakan jumlah data yang telah dipotong sesuai dengan nilai cutoff. Hasil waktu eksekusi kemudian disimpan dalam kamus store_many_timings sesuai dengan metodenya. Selain itu, hasil waktu eksekusi juga dicetak untuk setiap metode dan nilai cutoff.
"""

import matplotlib.pyplot as plt

def plot_with_legend(
    x_range, y_data, legend_labels, x_label, y_label, title, log=False
):
    """ Displays a single plot with multiple datasets and matching legends.
        Parameters:
        --------------
        x_range         list of lists containing x data
        y_data          list of lists containing y values
        legend_labels   list of string legend labels
        x_label         x axis label
        y_label         y axis label
    """
    plt.style.use("seaborn-whitegrid")
    plt.figure(figsize=(10, 7))

    if len(y_data) != len(legend_labels):
        raise TypeError(
            "Error: number of data sets does not match number of labels."
        )

    all_plots = []
    for data, label in zip(y_data, legend_labels):
        if log:
            temp, = plt.loglog(x_range, data, label=label)
        else:
            temp, = plt.plot(x_range, data, label=label)
        all_plots.append(temp)

    plt.title(title)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.legend(handles=all_plots)
    plt.show()

# Getting the store timings data to display
disk_x = store_many_timings["disk"]
lmdb_x = store_many_timings["lmdb"]
hdf5_x = store_many_timings["hdf5"]

plot_with_legend(
    cutoffs,
    [disk_x, lmdb_x, hdf5_x],
    ["PNG files", "LMDB", "HDF5"],
    "Number of images",
    "Seconds to store",
    "Storage time",
    log=False,
)

plot_with_legend(
    cutoffs,
    [disk_x, lmdb_x, hdf5_x],
    ["PNG files", "LMDB", "HDF5"],
    "Number of images",
    "Seconds to store",
    "Log storage time",
    log=True,
)

"""Berdasarkan grafik tersebut, menyimpan gambar dalam format PNG adalah yang paling lambat. Sementara itu, baik LMDB maupun HDF5 jauh lebih cepat. Meskipun HDF5 agak lambat untuk jumlah gambar kecil, namun LMDB lebih unggul untuk jumlah gambar yang lebih besar. Secara umum, LMDB adalah bisa jadi pilihan terbaik untuk menyimpan banyak gambar dari ketiga opsi yang ada.

##Reading a Single Image

###Reading From Disk
"""

def read_single_disk(image_id):
    """ Stores a single image to disk.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    image = np.array(Image.open(disk_dir / f"{image_id}.png"))

    with open(disk_dir / f"{image_id}.csv", "r") as csvfile:
        reader = csv.reader(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        label = int(next(reader)[0])

    return image, label

"""Fungsi `read_single_disk` digunakan untuk membaca satu gambar dari disk. Pertama, fungsi ini mengambil gambar dengan menggunakan `Image.open` dari modul PIL dan menyimpannya dalam bentuk array menggunakan `np.array`. Selanjutnya, fungsi membuka file CSV yang berisi label terkait dengan gambar tersebut, kemudian membacanya dan menyimpannya sebagai variabel `label`. Terakhir, fungsi mengembalikan array gambar dan label sebagai hasil pembacaan.

Untuk melakukan pengukuran waktu eksekusi dari fungsi `read_single_disk`, kita membuat kamus `_read_single_funcs` yang berisi fungsi `read_single_disk` dengan kunci "disk". Kemudian, dilakukan pengukuran waktu eksekusi untuk fungsi `read_single_disk` dengan menggunakan beberapa nilai `image_id` yang berbeda.

Setelah itu, dilakukan iterasi terhadap nilai-nilai `image_id` yang mungkin merupakan suatu variabel yang didefinisikan sebelumnya. Loop kedua akan melakukan iterasi terhadap metode pembacaan, yaitu "disk".

Kemudian, dilakukan pengukuran waktu eksekusi untuk fungsi `read_single_disk` dengan menggunakan nilai `image_id` tertentu. Hasil waktu eksekusi kemudian disimpan dalam kamus `read_single_timings` sesuai dengan metodenya. Selain itu, hasil waktu eksekusi juga dicetak untuk setiap metode dan nilai `image_id`.

Dengan demikian, kode tersebut digunakan untuk mengukur waktu eksekusi dari fungsi `read_single_disk` dengan menggunakan beberapa nilai `image_id` yang berbeda.

###Reading From LMDB
"""

def read_single_lmdb(image_id):
    """ Stores a single image to LMDB.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    # Open the LMDB environment
    env = lmdb.open(str(lmdb_dir / f"single_lmdb"), readonly=True)

    # Start a new read transaction
    with env.begin() as txn:
        # Encode the key the same way as we stored it
        data = txn.get(f"{image_id:08}".encode("ascii"))
        # Remember it's a CIFAR_Image object that is loaded
        cifar_image = pickle.loads(data)
        # Retrieve the relevant bits
        image = cifar_image.get_image()
        label = cifar_image.label
    env.close()

    return image, label

"""Fungsi `read_single_lmdb` digunakan untuk membaca satu gambar dari LMDB (Lightning Memory-Mapped Database). Pertama, fungsi membuka lingkungan LMDB dengan mode hanya baca (`readonly=True`) menggunakan `lmdb.open`. Selanjutnya, dilakukan transaksi baca baru dengan menggunakan `with env.begin() as txn:` untuk memulai transaksi baca pada lingkungan LMDB. Kunci (key) dienkoding dengan cara yang sama seperti saat disimpan sebelumnya, dan data yang sesuai dengan kunci tersebut diambil dari transaksi menggunakan `txn.get(f"{image_id:08}".encode("ascii"))`. Data yang diambil kemudian di-decode menggunakan `pickle.loads(data)` karena data yang disimpan dalam LMDB telah di-serialize sebelumnya. Dari objek CIFAR_Image yang di-load, gambar dan label yang relevan diambil. Setelah selesai membaca data, lingkungan LMDB ditutup. Terakhir, fungsi mengembalikan array gambar dan label sebagai hasil pembacaan dari LMDB. Dengan demikian, fungsi `read_single_lmdb` digunakan untuk membaca satu gambar dari LMDB dengan langkah-langkah yang jelas dan mudah dimengerti.

###Reading From HDF5
"""

def read_single_hdf5(image_id):
    """ Stores a single image to HDF5.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    # Open the HDF5 file
    file = h5py.File(hdf5_dir / f"{image_id}.h5", "r+")

    image = np.array(file["/image"]).astype("uint8")
    label = int(np.array(file["/meta"]).astype("uint8"))

    return image, label

"""Fungsi `read_single_hdf5` digunakan untuk membaca satu gambar dari file HDF5. Pertama, fungsi membuka file HDF5 menggunakan `h5py.File` dengan mode "r+" yang berarti file dibuka untuk dibaca dan ditulis. Selanjutnya, array gambar diambil dari dataset "/image" dalam file HDF5 menggunakan `np.array(file["/image"]).astype("uint8")`. Kemudian, array tersebut dikonversi ke tipe data "uint8". Label juga diambil dari dataset "/meta" dalam file HDF5 dengan cara yang serupa, yaitu `int(np.array(file["/meta"]).astype("uint8"))`. Setelah selesai membaca data, file HDF5 ditutup. Terakhir, fungsi mengembalikan array gambar dan label sebagai hasil pembacaan dari file HDF5. Dengan demikian, fungsi `read_single_hdf5` digunakan untuk membaca satu gambar dari file HDF5 dengan langkah-langkah yang jelas dan mudah dimengerti."""

_read_single_funcs = dict(
    disk=read_single_disk, lmdb=read_single_lmdb, hdf5=read_single_hdf5
)

"""###Experiment for Reading a Single Image"""

from timeit import timeit

read_single_timings = dict()

for method in ("disk", "lmdb", "hdf5"):
    t = timeit(
        "_read_single_funcs[method](0)",
        setup="image=images[0]; label=labels[0]",
        number=1,
        globals=globals(),
    )
    read_single_timings[method] = t
    print(f"Method: {method}, Time usage: {t}")

"""Kode di atas digunakan untuk mengukur waktu eksekusi dari fungsi pembaca gambar (`read_single_disk`, `read_single_lmdb`, `read_single_hdf5`) dengan menggunakan nilai indeks 0. Pertama, sebuah kamus bernama `read_single_timings` dibuat untuk menyimpan hasil waktu eksekusi dari setiap metode pembaca gambar. Selanjutnya, dilakukan iterasi melalui metode pembaca gambar yaitu "disk", "lmdb", dan "hdf5". Untuk setiap metode, dilakukan pengukuran waktu eksekusi dengan menggunakan `timeit`. Pada setiap iterasi, fungsi `_read_single_funcs[method](0)` dipanggil untuk mengukur waktu eksekusi dari metode tersebut terhadap gambar dengan indeks 0. Setup dilakukan dengan menginisialisasi variabel `image` dan `label` dengan nilai gambar dan label pada indeks 0 dari data `images` dan `labels`. Pengukuran waktu eksekusi dilakukan hanya sekali (`number=1`) untuk mendapatkan estimasi waktu eksekusi yang lebih akurat. Hasil waktu eksekusi dari setiap metode kemudian disimpan dalam kamus `read_single_timings` sesuai dengan metodenya. Selain itu, hasil waktu eksekusi juga dicetak untuk setiap metode berserta nilai waktu eksekusi yang didapatkan. Dengan demikian, kode tersebut digunakan untuk mengukur waktu eksekusi dari fungsi pembaca gambar dengan menggunakan nilai indeks 0 dan menyimpan hasil waktu eksekusi dari masing-masing metode pembaca gambar.

##Reading Many Images

###Adjusting the Code for Many Images
"""

def read_many_disk(num_images):
    """ Reads image from disk.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []

    # Loop over all IDs and read each image in one by one
    for image_id in range(num_images):
        images.append(np.array(Image.open(disk_dir / f"{image_id}.png")))

    with open(disk_dir / f"{num_images}.csv", "r") as csvfile:
        reader = csv.reader(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        for row in reader:
            labels.append(int(row[0]))
    return images, labels

def read_many_lmdb(num_images):
    """ Reads image from LMDB.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []
    env = lmdb.open(str(lmdb_dir / f"{num_images}_lmdb"), readonly=True)

    # Start a new read transaction
    with env.begin() as txn:
        # Read all images in one single transaction, with one lock
        # We could split this up into multiple transactions if needed
        for image_id in range(num_images):
            data = txn.get(f"{image_id:08}".encode("ascii"))
            # Remember that it's a CIFAR_Image object
            # that is stored as the value
            cifar_image = pickle.loads(data)
            # Retrieve the relevant bits
            images.append(cifar_image.get_image())
            labels.append(cifar_image.label)
    env.close()
    return images, labels

def read_many_hdf5(num_images):
    """ Reads image from HDF5.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []

    # Open the HDF5 file
    file = h5py.File(hdf5_dir / f"{num_images}_many.h5", "r+")

    images = np.array(file["/images"]).astype("uint8")
    labels = np.array(file["/meta"]).astype("uint8")

    return images, labels

_read_many_funcs = dict(
    disk=read_many_disk, lmdb=read_many_lmdb, hdf5=read_many_hdf5
)

"""Kode di atas mengimplementasikan tiga fungsi—disk, LMDB, dan HDF5—untuk membaca banyak gambar dari berbagai sumber.

Pertama, parameter "num_images" digunakan oleh fungsi "read_many_disk(num_images)" untuk mengambil jumlah gambar yang ingin dibaca. Kemudian fungsi ini membuat dua list kosong, salah satunya untuk menyimpan gambar ("images") dan yang lainnya untuk menyimpan label yang terkait. Selanjutnya, fungsi melakukan loop sebanyak "num_images", membuka setiap gambar dari direktori disk tertentu dan menyimpannya dalam bentuk array menggunakan library NumPy. Kemudian, fungsi membaca file CSV yang berisi label label untuk gambar-gambar tersebut dan menyimpannya dalam list "labels". Akhir sekali, fungsi mengembalikan tuple yang berisi array label dan array gambar.

Kedua, fungsi `read_many_lmdb(num_images)` juga mengambil jumlah gambar sebagai parameter. Fungsi ini memulai dengan membuat list kosong untuk gambar (`images`) dan label (`labels`). Selanjutnya, fungsi membuka lingkungan (environment) LMDB dengan mode baca saja (readonly) menggunakan pustaka LMDB. Dalam satu transaksi baca (read transaction) baru, fungsi membaca semua gambar sekaligus dari LMDB dengan satu kunci (key) tunggal, yang kemudian dimasukkan ke dalam list `images`. Selain itu, fungsi juga membaca label-label yang terkait dengan gambar-gambar tersebut dan menyimpannya dalam list `labels`. Setelah selesai, lingkungan LMDB ditutup.

Ketiga, fungsi `read_many_hdf5(num_images)` juga menerima jumlah gambar sebagai parameter. Fungsi ini membuka file HDF5 yang sesuai dengan jumlah gambar yang ingin dibaca. Kemudian, fungsi membaca array gambar (`images`) dan label-label (`labels`) dari file HDF5 yang sesuai dengan grup/grup-meta yang telah ditentukan. Setelah membaca data, fungsi mengembalikan tuple berisi array gambar (`images`) dan array label (`labels`).

###Experiment for Reading Many Images
"""

from timeit import timeit

read_many_timings = {"disk": [], "lmdb": [], "hdf5": []}

for cutoff in cutoffs:
    for method in ("disk", "lmdb", "hdf5"):
        t = timeit(
            "_read_many_funcs[method](num_images)",
            setup="num_images=cutoff",
            number=1,
            globals=globals(),
        )
        read_many_timings[method].append(t)

        # Print out the method, cutoff, and elapsed time
        print(f"Method: {method}, No. images: {cutoff}, Time usage: {t}")

"""Kode ini digunakan untuk menghitung waktu yang diperlukan untuk membaca banyak gambar sekaligus dari tiga metode penyimpanan yang berbeda: disk, LMDB (Database yang dimapatkan dengan Memori Bercahaya), dan versi 5 Format Data Hierarki. Eksperimen ini menggunakan variabel cutoff, yang menentukan jumlah gambar yang akan dibaca dalam setiap iterasi, untuk mengiterasi loop nested. Dalam setiap iterasi, juga terdapat loop yang mengiterasi melalui metode penyimpanan yang ada saat ini. Setiap metode penyimpanan membaca jumlah gambar yang ditentukan, dan fungsi timeit digunakan untuk menghitung waktu eksekusi pembacaan. Hasil penggunaan fungsi timeit untuk menghitung waktu pembacaan untuk setiap metode penyimpanan dan jumlah gambar yang ditentukan dicatat dalam dictionary read_many_timings. Hasil percobaan menunjukkan bahwa waktu pembacaan meningkat seiring dengan jumlah gambar yang dibaca, dan bahwa ada perbedaan antara berbagai metode penyimpanan. Metode pembacaan dari LMDB cenderung lebih cepat dibandingkan dengan metode pembacaan dari disk dan HDF5, terutama ketika jumlah gambar meningkat. Namun, ketika jumlah gambar mencapai 100,000, metode pembacaan dari HDF5 menunjukkan waktu eksekusi yang lebih lama. Ini menunjukkan bahwa kinerja pembacaan banyak gambar bergantung pada jenis penyimpanan yang digunakan dan pilihan metode penyimpanan.

"""